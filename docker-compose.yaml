version: '2'

services:
  kafka1:
    image: confluentinc/cp-kafka:latest
    container_name: kafka1
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka1:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    environment:
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka1:9093,2@kafka2:9093,3@kafka3:9093"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: "Mk3OEYBSD34fcwNTJENDM2Qk"
      KAFKA_LOG_CLEANUP_POLICY: delete  # Set log cleanup policy to delete
      KAFKA_LOG_RETENTION_MINUTES: 3
      KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS: 120000
    ports:
      - "9092:9092"
      - "9093:9093"

  kafka2:
    image: confluentinc/cp-kafka:latest
    container_name: kafka2
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka2:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    environment:
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 2
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka1:9093,2@kafka2:9093,3@kafka3:9093"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:9092
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: "Mk3OEYBSD34fcwNTJENDM2Qk"
      KAFKA_LOG_CLEANUP_POLICY: delete  # Set log cleanup policy to delete
      KAFKA_LOG_RETENTION_MINUTES: 3
      KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS: 120000
    ports:
      - "9094:9092"
      - "9095:9093"

  kafka3:
    image: confluentinc/cp-kafka:latest
    container_name: kafka3
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka3:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    environment:
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 3
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka1:9093,2@kafka2:9093,3@kafka3:9093"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:9092
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: "Mk3OEYBSD34fcwNTJENDM2Qk"
      KAFKA_LOG_CLEANUP_POLICY: delete  # Set log cleanup policy to delete
      KAFKA_LOG_RETENTION_MINUTES: 3
      KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS: 120000
    ports:
      - "9096:9092"
      - "9097:9093"

  kafka4:
    image: confluentinc/cp-kafka:latest
    container_name: kafka4
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka4:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    environment:
      KAFKA_PROCESS_ROLES: broker
      KAFKA_NODE_ID: 4
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka1:9093,2@kafka2:9093,3@kafka3:9093"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka4:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: "Mk3OEYBSD34fcwNTJENDM2Qk"
      KAFKA_LOG_CLEANUP_POLICY: delete  # Set log cleanup policy to delete
      KAFKA_LOG_RETENTION_MINUTES: 3
      KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS: 120000
    ports:
      - "9098:9092"

  kafka5:
    image: confluentinc/cp-kafka:latest
    container_name: kafka5
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka5:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    environment:
      KAFKA_PROCESS_ROLES: broker
      KAFKA_NODE_ID: 5
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka1:9093,2@kafka2:9093,3@kafka3:9093"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka5:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: "Mk3OEYBSD34fcwNTJENDM2Qk"
      KAFKA_LOG_CLEANUP_POLICY: delete  # Set log cleanup policy to delete
      KAFKA_LOG_RETENTION_MINUTES: 3
      KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS: 120000
    ports:
      - "9099:9092"
  
  kafka-init-topics:
    image: confluentinc/cp-kafka:latest
    command: [ "/bin/bash", "-c", "/create_topic.sh"]
    volumes:
      - type: bind
        source: ./producer/create_topic.sh
        target: /create_topic.sh
    depends_on:
      kafka1:
        condition: service_healthy
    init: true


  producer:
    build: ./producer
    depends_on:
      kafka1:
        condition: service_healthy
      kafka2:
        condition: service_healthy
      kafka3:
        condition: service_healthy
      kafka4:
        condition: service_healthy
      kafka5:
        condition: service_healthy
      job-submitter:
        condition: service_completed_successfully
    volumes:
      - ./producer/data:/app/data
    command: sh -c "sleep 10 && python main.py"
    environment:
      BATCH_SIZE: 10000

  timescaledb:
    build:
      context: ./database
    environment:
      - POSTGRES_PASSWORD=password
      - POSTGRES_USER=postgres
      - POSTGRES_DB=stocksdb
    ports:
      - "5433:5432"
    volumes:
      - ./database:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  python-db-helper:
    build:
      context: ./python-db-helper
    depends_on:
      timescaledb:
        condition: service_healthy
    environment:
      - DB_HOST=timescaledb
      - DB_PORT=5432
      - DB_USER=postgres
      - DB_PASS=password
      - DB_NAME=stocksdb
    volumes:
      - ./python-db-helper:/app
    command: ["python", "populate_stocks.py"]

  fastapi:
     container_name: datamanagement-fastapi-1
     build:
       context: ./timescale-fastapi
     depends_on:
       timescaledb:
         condition: service_healthy
     environment:
       - DB_HOST=timescaledb
       - DB_PORT=5432
       - DB_USER=postgres
       - DB_PASS=password
       - DB_NAME=stocksdb
     ports:
       - "8000:8000"
     volumes:
       - ./timescale-fastapi:/usr/app
     command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

  jobmanager:
    image: flink:1.20-scala_2.12
    build: ./consumer
    hostname: "jobmanager"
    expose:
      - "6123"
    ports:
      - "8081:8081"
    command: jobmanager 
    volumes:
      - ./consumer/flink_job_ema.py:/opt/flink_job_ema.py  
      - ./consumer/flink_job_ema.py:/opt/flink_job_price.py
      - ./consumer/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml

  taskmanager:
    image: flink:1.20-scala_2.12
    depends_on:
      - jobmanager
    command: taskmanager
    scale: 1
    volumes:
      - ./consumer/flink_job_ema.py:/opt/flink_job_ema.py
      - ./consumer/flink_job_ema.py:/opt/flink_job_price.py
      - ./consumer/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml

  job-submitter:
    image: flink:1.20-scala_2.12
    depends_on:
      kafka1:
        condition: service_healthy
      jobmanager:
        condition: service_started
    volumes:
      - ./consumer/submit_job.sh:/opt/submit_job.sh
      - ./consumer/flink_job_ema.py:/opt/flink_job_ema.py
      - ./consumer/flink_job_price.py:/opt/flink_job_price.py
      - ./consumer/flink_job_topic_read.py:/opt/flink_job_topic_read.py
      - ./consumer/flink_job_same_window.py:/opt/flink_job_same_window.py
    entrypoint: ["/bin/bash", "/opt/submit_job.sh"]

  redis:
    image: redis:alpine
    ports:
      - 6379:6379
  
  grafana:
    image: grafana/grafana:10.0.2
    ports:
      - "3000:3000"
    volumes:
      - ./frontend/provisioning:/etc/grafana/provisioning
      - ./frontend/Financial Data Analysis.json:/etc/grafana/provisioning/dashboards/dashboard.json
    environment:
      - GF_INSTALL_PLUGINS=yesoreyeram-infinity-datasource
      - GF_DATE_FORMATS_FULL_DATE=YYYY-MM-DD HH:mm:ss.SSS

      # Enable anonymous access
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_NAME=Main Org.
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
      
      # Default admin credentials (for admin access if needed)
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin